{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"7703a4a8-642e-4478-8129-9dcef80376f0","cell_type":"markdown","source":"# Preparing data","metadata":{}},{"id":"560453c2-aa3c-441d-8d97-af8e1eb82bd3","cell_type":"code","source":"%pip install -q pytorch-crf seqeval","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3119b943-3d6c-4f58-a272-8dc778dd1c2b","cell_type":"code","source":"!git clone https://github.com/bborisggg/chinese_word_segmentation.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b123b6e3-9343-4683-a6a3-da644da38f8f","cell_type":"code","source":"import os\nimport sys\nsys.path.insert(1, '/kaggle/working/chinese_word_segmentation/')\nos.chdir('/kaggle/working/chinese_word_segmentation/')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e49f19df-3a32-44f6-ba70-4b7f68e8b096","cell_type":"code","source":"import codecs\nimport argparse\nimport pickle\nimport warnings\nimport collections\nfrom utils import get_processing_word, read_pretrained_embeddings, is_dataset_tag, make_sure_path_exists, to_id_list\nfrom convert_corpus import convert_corpus\nfrom seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments,BertConfig\nimport numpy as np\nfrom tqdm import tqdm\nfrom torchcrf import CRF\nfrom copy import deepcopy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cca32d88-8cfc-4404-8d07-5e2a93682b38","cell_type":"code","source":"convert_corpus()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"de69a5b1-f57f-4bb3-bc12-8ec851aec13b","cell_type":"code","source":"Instance = collections.namedtuple(\"Instance\", [\"sentence\", \"tags\"])\n\nUNK_TAG = \"<UNK>\"\nNONE_TAG = \"<NONE>\"\nSTART_TAG = \"<START>\"\nEND_TAG = \"<STOP>\"\nPADDING_CHAR = \"<*>\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e7c1ce4f-659f-4ceb-9ec9-f5d994a9e30b","cell_type":"code","source":"def read_file(filename, w2i, t2i, c2i, max_iter=sys.maxsize, processing_word=get_processing_word(lowercase=False)):\n    \"\"\"\n    Read in a dataset and turn it into a list of instances.\n    Modifies the w2i, t2is and c2i dicts, adding new words/attributes/tags/chars \n    as it s\n    ees them.\n    \"\"\"\n    instances = []\n    vocab_counter = collections.Counter()\n    niter = 0\n    with codecs.open(filename, \"r\", \"utf-8\") as f:\n        words, tags = [], []\n        for line in f:\n            line = line.strip()\n            if len(line) == 0 or line.startswith(\"-DOCSTART-\"):\n                if len(words) != 0:\n                    niter += 1\n                    if max_iter is not None and niter > max_iter:\n                        break\n                    instances.append(Instance(words, tags))\n                    words, tags = [], []\n            else:\n                word, tag = line.split()\n                word = processing_word(word)\n                vocab_counter[word] += 1\n                if word not in w2i:\n                    w2i[word] = len(w2i)\n                if tag not in t2i:\n                    t2i[tag] = len(t2i)\n                if is_dataset_tag(word):\n                    if word not in c2i:\n                        c2i[word] = len(c2i)\n                else:\n                    for c in word:\n                        if c not in c2i:\n                            c2i[c] = len(c2i)\n                words.append(w2i[word])\n                tags.append(t2i[tag])\n    return instances, vocab_counter","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"58aa3491-237d-4a9a-a72c-757325b6a273","cell_type":"code","source":"options = {'training_data':'./data/ctb/bmes/train-all.txt',\n          'dev_data':'./data/ctb/bmes/dev.txt',\n          'test_data':'./data/ctb/bmes/test.txt',\n          'output':'dataset/ctb/dataset.pkl'}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4cf0bf2e-1416-406f-879b-0a040e6533bc","cell_type":"code","source":"w2i = {}  # mapping from word to index\nt2i = {}  # mapping from tag to index\nc2i = {}\n\nprint('Making training dataset')\ntraining_instances, training_vocab = read_file(options['training_data'], w2i, t2i, c2i)\nprint('Making dev dataset')\ndev_instances, dev_vocab = read_file(options['dev_data'], w2i, t2i, c2i)\nprint('Making test dataset')\ntest_instances, test_vocab = read_file(options['test_data'], w2i, t2i, c2i)\n\n# Add special tokens / tags / chars to dicts\nw2i[UNK_TAG] = len(w2i)\nt2i[START_TAG] = len(t2i)\nt2i[END_TAG] = len(t2i)\nc2i[UNK_TAG] = len(c2i)\n\n\ni2w = to_id_list(w2i)  # Inverse mapping\ni2t = to_id_list(t2i)\ni2c = to_id_list(c2i)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7f44cbd3-c933-4458-bc7a-4c63ce1a3e97","cell_type":"code","source":"sum([len(el.sentence) for el in test_instances])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"344d17c3-2734-4287-8d4c-32cf742bd496","cell_type":"code","source":"max([len(i.sentence) for i in training_instances])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"8437e074-5466-4a94-8dd9-71936c20d733","cell_type":"code","source":"MAX_LENGTH=128","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0f20581e-40a7-46bd-9809-66f364bed1b5","cell_type":"code","source":"num_labels = len(i2t)\n\n# Create a custom Dataset\nclass WordSegmentationDataset(Dataset):\n    def __init__(self, data, max_length=MAX_LENGTH, padding_tag = -100):\n        self.data = data\n        self.max_length = max_length\n        self.padding_tag = padding_tag\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        input_ids = self.data[idx].sentence\n        labels = self.data[idx].tags\n        length = len(input_ids)\n        # Padding\n        padding_length = self.max_length - len(input_ids)\n        if padding_length > 0:\n            input_ids = input_ids + ([0] * padding_length)\n            labels = labels + ([self.padding_tag] * padding_length)  # -100 will be ignored in loss calculation\n            attention_mask = [1] * (self.max_length -padding_length) + ([0] * padding_length)\n        else:\n            input_ids = input_ids[:self.max_length]\n            labels = labels[:self.max_length]\n            attention_mask = [1] * self.max_length\n        \n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'labels': torch.tensor(labels, dtype=torch.long),\n            'length': length\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7cfd709c-cdeb-48b7-b605-9376d3e727b6","cell_type":"code","source":"train_dataset = WordSegmentationDataset(training_instances)\nval_dataset = WordSegmentationDataset(dev_instances)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"57fcbeca-f11a-4dc8-bc8d-531e87ea4598","cell_type":"markdown","source":"# BERT model","metadata":{}},{"id":"0c550274-f8f2-44fe-b9e6-e301c8869293","cell_type":"code","source":"config = BertConfig(\n    vocab_size=len(i2c)+1,  # +1 for padding token\n    hidden_size=256,\n    num_hidden_layers=4,\n    num_attention_heads=8,\n    intermediate_size=512,\n    max_position_embeddings=MAX_LENGTH,\n    num_labels=num_labels,\n    pad_token_id=0\n)\n# Initialize the model\nmodel = BertForTokenClassification(config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"613a9e8b-bca3-47df-a23d-db551aa56087","cell_type":"code","source":"# Define compute_metrics function for evaluation\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n    \n    true_labels = []\n    true_predictions = []\n\n    for prediction, label in zip(predictions, labels):\n        temp_labels = []\n        temp_preds = []\n\n        for pred, lbl in zip(prediction, label):\n            if lbl != -100:\n                temp_labels.append(i2t[lbl])\n                temp_preds.append(i2t[pred])\n\n        true_labels.append(temp_labels)\n        true_predictions.append(temp_preds)\n    \n    return {\n        \"accuracy\": accuracy_score(true_labels, true_predictions),\n        \"precision\": precision_score(true_labels, true_predictions, average='macro'),\n        \"recall\": recall_score(true_labels, true_predictions, average='macro'),\n        \"f1\": f1_score(true_labels, true_predictions, average='macro'),\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7fe06934-9c2e-41f9-8d85-6e2d664ab593","cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\"\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ed9b7649-61cb-4e61-ac94-db942482d233","cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    report_to=\"none\",\n    output_dir='./results',\n    num_train_epochs=10,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    logging_steps=500,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,\n    save_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    load_best_model_at_end=True,\n    metric_for_best_model='f1',\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)\n\n# Start Training\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4ea08216-ce6f-4c5e-b4bc-0c4f896f779c","cell_type":"code","source":"bert_model = model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"11e4324c-88e6-4401-95cd-ea64172fd39c","cell_type":"markdown","source":"# Bi-LSTM","metadata":{}},{"id":"dc751392-41df-4b2e-b6ac-4b74ac5c6b35","cell_type":"code","source":"# Create DataLoaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e549b9b0-aefd-4a09-83cd-a0036d45b2b7","cell_type":"code","source":"# Define the BiLSTM model\nclass BiLSTMTagger(nn.Module):\n       def __init__(self, vocab_size, tagset_size, embedding_dim=256, hidden_dim=256, pad_token_id=0):\n           super(BiLSTMTagger, self).__init__()\n           self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_token_id)\n           self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, \n                               bidirectional=True, batch_first=True)\n           self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n       \n       def forward(self, input_ids, attention_mask, lengths):\n           embeds = self.embedding(input_ids)\n           packed_input = nn.utils.rnn.pack_padded_sequence(embeds, lengths, \n                                                            batch_first=True, enforce_sorted=False)\n           packed_output, _ = self.lstm(packed_input)\n           # Use total_length to ensure consistent sequence length\n           lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True, \n                                                          total_length=input_ids.size(1))\n           tag_space = self.hidden2tag(lstm_out)\n           return tag_space","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"372e0940-d608-4879-a4cc-c91ddc0634f8","cell_type":"code","source":"# Initialize the model\nvocab_size = len(i2c) + 1  # +1 for padding token\ntagset_size = num_labels\npad_token_id = -100","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"669fb972-0bd0-45fa-8e69-b977f79b326f","cell_type":"code","source":"model = BiLSTMTagger(vocab_size, tagset_size, pad_token_id=pad_token_id)\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"986d5250-0486-4346-9e0d-401ee500c814","cell_type":"code","source":"num_epochs = 10\nbest_f1 = 0\nbest_state = None\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        lengths = batch['length']\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, lengths)\n\n        # Reshape outputs and labels for computing loss\n        outputs = outputs.view(-1, tagset_size)\n        labels = labels.view(-1)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n    \n    # Evaluation\n    model.eval()\n    true_labels = []\n    true_predictions = []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            lengths = batch['lengths']\n            \n            outputs = model(input_ids, attention_mask, lengths)\n            \n            # Get predictions\n            _, preds = torch.max(outputs, dim=2)\n            \n            preds = preds.cpu().numpy()\n            labels = labels.cpu().numpy()\n            \n            for pred, label, mask in zip(preds, labels, attention_mask.cpu().numpy()):\n                temp_labels = []\n                temp_preds = []\n                \n                for i in range(len(pred)):\n                    if mask[i]:\n                        temp_labels.append(i2t[label[i]])\n                        temp_preds.append(i2t[pred[i]])\n                true_labels.append(temp_labels)\n                true_predictions.append(temp_preds)\n    acc = accuracy_score(true_labels, true_predictions)\n    prec = precision_score(true_labels, true_predictions, average='macro')\n    rec = recall_score(true_labels, true_predictions, average='macro')\n    f1 = f1_score(true_labels, true_predictions, average='macro')\n    print(f\"Validation Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-score: {f1:.4f}\\n\")\n    if f1 > best_f1:\n        best_state = deepcopy(model.state_dict())\n        best_f1 = f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"104aa982-ee27-4d6d-a0c7-ac0d08437abb","cell_type":"code","source":"model.load_state_dict(best_state)  \nbilstm_model = model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5f3f6bff-323f-4bf5-9601-c796b462f7b7","cell_type":"markdown","source":"# BERT-BiLSTM-CRF","metadata":{}},{"id":"3efc59a7-2111-474e-830a-928b14db9fae","cell_type":"code","source":"# We redifine loss calculation in order to correct for padding\ntrain_dataset = WordSegmentationDataset(training_instances, padding_tag=1)\nval_dataset = WordSegmentationDataset(dev_instances, padding_tag=1)\n\n# Create DataLoaders\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"73594d3e-7331-4a3d-a787-4b81f648944d","cell_type":"code","source":"class BertBiLSTMCRF(nn.Module):\n    def __init__(self, num_labels, vocab_size=30522,hidden_size=256,num_hidden_layers=4,\n                 num_attention_heads=8,intermediate_size=512,max_position_embeddings=MAX_LENGTH,\n                 hidden_dim_lstm=256, num_lstm_layers=1, pad_token=0):\n        super().__init__()\n        config = BertConfig(\n            vocab_size=vocab_size, hidden_size=hidden_size, num_hidden_layers=num_hidden_layers,\n            num_attention_heads=num_attention_heads, intermediate_size=intermediate_size,\n            pad_token_id=pad_token\n        )\n        self.bert = BertModel(config)\n        self.lstm = nn.LSTM(\n            input_size=hidden_size, hidden_size=hidden_dim_lstm // 2,\n            num_layers=num_lstm_layers, bidirectional=True, batch_first=True\n        )\n        self.fc = nn.Linear(hidden_dim_lstm, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        lstm_out, _ = self.lstm(outputs.last_hidden_state)\n        emissions = self.fc(lstm_out)\n        if labels is not None:\n            loss = -self.crf(emissions, labels, mask=attention_mask.bool(), reduction='mean')\n            return loss\n        else:\n            return self.crf.decode(emissions, mask=attention_mask.bool())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"abc24e15-3204-4606-b812-4d60fde503c0","cell_type":"code","source":"from transformers import BertModel, BertConfig\nfrom tqdm import tqdm\n\nmodel = BertBiLSTMCRF(num_labels=tagset_size, vocab_size=len(i2c)+1, pad_token=1)\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)\nnum_epochs=10\nbest_f1 = 0\nbest_state = None\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        loss = model(input_ids, attention_mask, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n\n\n    model.eval()\n    true_labels = []\n    true_predictions = []\n    for batch in tqdm(val_loader):\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels']\n            \n            preds = model(input_ids, attention_mask)\n            \n            \n            for pred, label, mask in zip(preds, labels, attention_mask.cpu().numpy()):\n                temp_labels = []\n                temp_preds = []\n                \n                for i in range(len(pred)):\n                    if mask[i]:\n                        temp_labels.append(i2t[label[i]])\n                        temp_preds.append(i2t[pred[i]])\n                true_labels.append(temp_labels)\n                true_predictions.append(temp_preds)\n    acc = accuracy_score(true_labels, true_predictions)\n    prec = precision_score(true_labels, true_predictions, average='macro')\n    rec = recall_score(true_labels, true_predictions, average='macro')\n    f1 = f1_score(true_labels, true_predictions, average='macro')\n    print(f\"Validation Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1-score: {f1:.4f}\\n\")\n    if f1 > best_f1:\n        best_state = deepcopy(model.state_dict())\n        best_f1 = f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cd8578a7-a713-40ab-b6ea-a8f3d7b15506","cell_type":"code","source":"model.load_state_dict(best_state)  \nbilstmbertcrf_model = model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ef76ee77-9411-4063-8b0c-e8f8e0ec20e4","cell_type":"markdown","source":"# Saving models","metadata":{}},{"id":"ba924c3f-36c0-42f2-a07b-a7d4149ecc20","cell_type":"code","source":"trainer.save_model(\"bert_model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"68fcc5c1-178e-4f01-acd4-0b61a677a6a4","cell_type":"code","source":"torch.save(bilstmbertcrf_model.state_dict(), \"bilstmbertcrf_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"405802e0-c6c5-4b7f-b484-4f4c000ad8f8","cell_type":"code","source":"torch.save(bilstm_model.state_dict(), \"bilstm_model.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}